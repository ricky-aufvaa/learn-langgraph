{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# LangGraph with Structured Output and Conditional Routing\n",
    "\n",
    "This notebook demonstrates advanced LangGraph concepts:\n",
    "- **Structured Output**: Using Pydantic models for type-safe LLM responses\n",
    "- **Conditional Routing**: Dynamic graph paths based on user intent\n",
    "- **Multiple Nodes**: Coordinating different processing functions\n",
    "- **LLM Integration**: Working with AWS Bedrock and Claude\n",
    "- **Decision Making**: Using LLMs to route between different workflows\n",
    "\n",
    "## Use Case: Intelligent Librarian System\n",
    "\n",
    "We'll build a system that can:\n",
    "1. **Identify Author**: Given a book name, find the author and provide author persona\n",
    "2. **Suggest Books**: Recommend similar books based on user preferences\n",
    "3. **Smart Routing**: Automatically determine which service the user needs\n",
    "\n",
    "This pattern is useful for:\n",
    "- **Multi-service Applications**: When you have different capabilities\n",
    "- **Intent Classification**: Routing users to appropriate handlers\n",
    "- **Structured Data Extraction**: Getting consistent, typed responses from LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## 1. Essential Imports and Setup\n",
    "\n",
    "Let's import all the necessary libraries for our advanced LangGraph application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import display, Image\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-definition",
   "metadata": {},
   "source": [
    "## 2. State Definition\n",
    "\n",
    "Our state needs to handle multiple types of data:\n",
    "- **messages**: For conversation flow\n",
    "- **book**: Book information\n",
    "- **author_name**: Author identification\n",
    "- **author_persona**: Author's personality/greeting\n",
    "- **suggestions**: Book recommendations\n",
    "\n",
    "### Design Considerations:\n",
    "- **Flexibility**: State should accommodate different workflows\n",
    "- **Type Safety**: Clear field definitions for better debugging\n",
    "- **Extensibility**: Easy to add new fields as features grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class BookState(MessagesState):\n",
    "    \"\"\"Enhanced state for our librarian system.\n",
    "    \n",
    "    Inherits from MessagesState to get message handling capabilities,\n",
    "    and adds book-specific fields for our use case.\n",
    "    \"\"\"\n",
    "    book: str = \"\"                    # Book title or query\n",
    "    author_name: str = \"\"             # Identified author\n",
    "    author_persona: str = \"\"          # Author's greeting/introduction\n",
    "    suggestions: list = []            # List of book suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-setup",
   "metadata": {},
   "source": [
    "## 3. LLM Setup with AWS Bedrock\n",
    "\n",
    "We'll use AWS Bedrock with Claude for our LLM capabilities.\n",
    "\n",
    "### Why AWS Bedrock?\n",
    "- **Enterprise Ready**: Built for production use\n",
    "- **Multiple Models**: Access to various LLMs\n",
    "- **Scalability**: Handles high-volume requests\n",
    "- **Security**: Enterprise-grade security features\n",
    "\n",
    "### Alternative LLM Providers:\n",
    "- **OpenAI**: `ChatOpenAI` for GPT models\n",
    "- **Anthropic**: Direct `ChatAnthropic` integration\n",
    "- **Local Models**: `Ollama` for on-premise deployment\n",
    "- **Azure**: `AzureChatOpenAI` for Microsoft cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"Initialize AWS Bedrock ChatBedrock model.\n",
    "    \n",
    "    This function sets up our connection to Claude via AWS Bedrock.\n",
    "    Make sure you have AWS credentials configured.\n",
    "    \n",
    "    Returns:\n",
    "        ChatBedrock: Configured LLM instance\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create Bedrock client with credentials\n",
    "    bedrock_client = boto3.client(\n",
    "        \"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\",\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Initialize ChatBedrock with Claude Sonnet\n",
    "    return ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-output",
   "metadata": {},
   "source": [
    "## 4. Structured Output with Pydantic\n",
    "\n",
    "**Structured Output** ensures LLMs return data in a consistent, typed format.\n",
    "\n",
    "### Benefits of Structured Output:\n",
    "- **Type Safety**: Guaranteed data structure\n",
    "- **Validation**: Automatic data validation\n",
    "- **Consistency**: Same format every time\n",
    "- **Integration**: Easy to use in downstream processing\n",
    "\n",
    "### Methods for Structured Output:\n",
    "1. **with_structured_output()**: Cleanest approach (recommended)\n",
    "2. **bind_tools()**: More control over tool calling\n",
    "3. **Output Parsers**: Manual parsing (less reliable)\n",
    "\n",
    "We'll use `with_structured_output()` as it's the most reliable method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Model for author identification responses\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Structured response for author identification queries.\"\"\"\n",
    "    author_name: str = Field(description=\"The author of the mentioned book\")\n",
    "    author_persona: str = Field(\n",
    "        description=\"A brief greeting/introduction from the author of the mentioned book\"\n",
    "    )\n",
    "\n",
    "# Model for book suggestion responses\n",
    "class SuggestionAnswer(BaseModel):\n",
    "    \"\"\"Structured response for book suggestion queries.\"\"\"\n",
    "    books_titles: list = Field(\n",
    "        description=\"A list of 3 books to be suggested based on user request\"\n",
    "    )\n",
    "\n",
    "# Model for decision making\n",
    "class Decision(BaseModel):\n",
    "    \"\"\"Decision model for routing user queries.\"\"\"\n",
    "    author: bool = Field(\n",
    "        description=\"True if the user query is about finding an author of a book\"\n",
    "    )\n",
    "    suggestion: bool = Field(\n",
    "        description=\"True if the user query is asking for book suggestions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompts-section",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering\n",
    "\n",
    "Well-crafted prompts are crucial for reliable LLM behavior.\n",
    "\n",
    "### Prompt Design Principles:\n",
    "- **Clear Instructions**: Specific about what you want\n",
    "- **Context Setting**: Establish the LLM's role\n",
    "- **Output Format**: Specify expected response structure\n",
    "- **Examples**: Show desired behavior when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for author identification\n",
    "AUTHOR_PROMPT = \"\"\"You are a knowledgeable librarian with access to vast literary knowledge.\n",
    "\n",
    "Your tasks:\n",
    "1. Identify the author of the book mentioned by the user\n",
    "2. Provide a brief, engaging greeting as if you were that author\n",
    "\n",
    "The greeting should:\n",
    "- Be written in first person as the author\n",
    "- Reflect the author's personality and writing style\n",
    "- Be warm and welcoming to the reader\n",
    "- Reference their notable works when appropriate\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for book suggestions\n",
    "SUGGESTION_PROMPT = \"\"\"You are a librarian who suggests books based on user requests.\n",
    "\n",
    "Your task:\n",
    "- Analyze the user's request for book recommendations\n",
    "- Suggest exactly 3 books that match their interests\n",
    "- Consider genre, themes, writing style, and popularity\n",
    "- Provide diverse recommendations when possible\n",
    "\n",
    "Focus on quality recommendations that truly match the user's expressed interests.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for decision making\n",
    "DECISION_PROMPT = \"\"\"Analyze the user's query and determine their intent.\n",
    "\n",
    "Classification rules:\n",
    "- Set 'author' to True if they're asking about who wrote a specific book\n",
    "- Set 'suggestion' to True if they want book recommendations or suggestions\n",
    "- Only one should be True based on the primary intent\n",
    "- If unclear, lean towards the most likely interpretation\n",
    "\n",
    "Examples:\n",
    "- \"Who wrote 1984?\" ‚Üí author: True, suggestion: False\n",
    "- \"Suggest books like Harry Potter\" ‚Üí author: False, suggestion: True\n",
    "- \"I need book recommendations\" ‚Üí author: False, suggestion: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chains-section",
   "metadata": {},
   "source": [
    "## 6. Building LangChain Chains\n",
    "\n",
    "**Chains** combine prompts with LLMs to create reusable processing units.\n",
    "\n",
    "### Chain Components:\n",
    "1. **Prompt Template**: Structures the input\n",
    "2. **LLM**: Processes the prompt\n",
    "3. **Output Parser**: Formats the response (handled by structured output)\n",
    "\n",
    "### Why Use Chains?\n",
    "- **Reusability**: Same logic across different contexts\n",
    "- **Modularity**: Easy to test and modify individual components\n",
    "- **Consistency**: Standardized processing patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_chain():\n",
    "    \"\"\"Create a chain for author identification with structured output.\n",
    "    \n",
    "    This chain:\n",
    "    1. Takes user messages about books\n",
    "    2. Identifies the author\n",
    "    3. Creates an author persona greeting\n",
    "    4. Returns structured AnswerQuestion object\n",
    "    \n",
    "    Returns:\n",
    "        Chain that outputs AnswerQuestion objects\n",
    "    \"\"\"\n",
    "    llm = get_chat_model()\n",
    "    \n",
    "    # Use structured output for reliable data format\n",
    "    structured_llm = llm.with_structured_output(AnswerQuestion)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", AUTHOR_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    return prompt | structured_llm\n",
    "\n",
    "def suggestion_chain():\n",
    "    \"\"\"Create a chain for book suggestions with structured output.\n",
    "    \n",
    "    This chain:\n",
    "    1. Analyzes user's book preferences\n",
    "    2. Generates 3 relevant book suggestions\n",
    "    3. Returns structured SuggestionAnswer object\n",
    "    \n",
    "    Returns:\n",
    "        Chain that outputs SuggestionAnswer objects\n",
    "    \"\"\"\n",
    "    llm = get_chat_model()\n",
    "    structured_llm = llm.with_structured_output(SuggestionAnswer)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SUGGESTION_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    return prompt | structured_llm\n",
    "\n",
    "def decision_chain():\n",
    "    \"\"\"Create a chain for intent classification.\n",
    "    \n",
    "    This chain determines whether the user wants:\n",
    "    - Author information (author=True)\n",
    "    - Book suggestions (suggestion=True)\n",
    "    \n",
    "    Returns:\n",
    "        Chain that outputs Decision objects\n",
    "    \"\"\"\n",
    "    llm = get_chat_model()\n",
    "    structured_llm = llm.with_structured_output(Decision)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", DECISION_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    return prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-chains",
   "metadata": {},
   "source": [
    "## 7. Testing Individual Chains\n",
    "\n",
    "Before building the full graph, let's test our chains individually to ensure they work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-chains",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Test the author chain\n",
    "print(\"Testing Author Chain:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "author_test_chain = author_chain()\n",
    "author_result = author_test_chain.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Who is the author of Khushwantnama?\")]\n",
    "})\n",
    "\n",
    "print(f\"Author: {author_result.author_name}\")\n",
    "print(f\"Persona: {author_result.author_persona}\")\n",
    "print(f\"Type: {type(author_result)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing Suggestion Chain:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "suggestion_test_chain = suggestion_chain()\n",
    "suggestion_result = suggestion_test_chain.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Suggest some books like Macbeth\")]\n",
    "})\n",
    "\n",
    "print(f\"Suggestions: {suggestion_result.books_titles}\")\n",
    "print(f\"Type: {type(suggestion_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nodes-section",
   "metadata": {},
   "source": [
    "## 8. Creating Graph Nodes\n",
    "\n",
    "**Nodes** are the processing units in our graph. Each node:\n",
    "1. Receives the current state\n",
    "2. Processes it using our chains\n",
    "3. Returns state updates\n",
    "\n",
    "### Critical Node Requirements:\n",
    "- **State Updates**: Must return dictionaries that update the state\n",
    "- **Error Handling**: Should handle chain failures gracefully\n",
    "- **Type Consistency**: Ensure returned data matches state expectations\n",
    "\n",
    "### Common Node Patterns:\n",
    "- **Processing Nodes**: Transform data using LLMs\n",
    "- **Decision Nodes**: Route based on conditions\n",
    "- **Integration Nodes**: Call external APIs or services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_node(state: BookState) -> BookState:\n",
    "    \"\"\"Process author identification requests.\n",
    "    \n",
    "    This node:\n",
    "    1. Uses the author chain to identify the book's author\n",
    "    2. Extracts structured data from the response\n",
    "    3. Updates the state with author information\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with user messages\n",
    "        \n",
    "    Returns:\n",
    "        State update with author_name and author_persona\n",
    "    \"\"\"\n",
    "    print(\"üìö Processing author identification request...\")\n",
    "    \n",
    "    chain = author_chain()\n",
    "    result = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    print(f\"‚úÖ Found author: {result.author_name}\")\n",
    "    \n",
    "    # Return state updates - this is crucial!\n",
    "    return {\n",
    "        \"author_name\": result.author_name,\n",
    "        \"author_persona\": result.author_persona\n",
    "    }\n",
    "\n",
    "def suggestion_node(state: BookState) -> BookState:\n",
    "    \"\"\"Process book suggestion requests.\n",
    "    \n",
    "    This node:\n",
    "    1. Uses the suggestion chain to generate book recommendations\n",
    "    2. Extracts the list of suggested books\n",
    "    3. Updates the state with suggestions\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with user messages\n",
    "        \n",
    "    Returns:\n",
    "        State update with book suggestions\n",
    "    \"\"\"\n",
    "    print(\"üìñ Processing book suggestion request...\")\n",
    "    \n",
    "    chain = suggestion_chain()\n",
    "    result = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(result.books_titles)} suggestions\")\n",
    "    \n",
    "    # Return state updates\n",
    "    return {\n",
    "        \"suggestions\": result.books_titles\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-routing",
   "metadata": {},
   "source": [
    "## 9. Conditional Routing Function\n",
    "\n",
    "This function determines which path the graph should take based on user intent.\n",
    "\n",
    "### Routing Strategies:\n",
    "1. **LLM-Based**: Use AI to classify intent (our approach)\n",
    "2. **Rule-Based**: Use keywords or patterns\n",
    "3. **Hybrid**: Combine both approaches\n",
    "4. **ML Classification**: Train a dedicated classifier\n",
    "\n",
    "### Why LLM-Based Routing?\n",
    "- **Flexibility**: Handles natural language variations\n",
    "- **Context Awareness**: Understands nuanced requests\n",
    "- **No Training Required**: Works out of the box\n",
    "- **Easy Updates**: Modify behavior through prompt changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "routing-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(state: BookState) -> str:\n",
    "    \"\"\"Determine routing based on user intent.\n",
    "    \n",
    "    This function:\n",
    "    1. Analyzes user messages using the decision chain\n",
    "    2. Classifies intent as author lookup or book suggestions\n",
    "    3. Returns routing decision for the graph\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with user messages\n",
    "        \n",
    "    Returns:\n",
    "        \"author\" for author queries, \"suggestion\" for book recommendations\n",
    "    \"\"\"\n",
    "    print(\"ü§î Analyzing user intent...\")\n",
    "    \n",
    "    chain = decision_chain()\n",
    "    decision = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    if decision.author:\n",
    "        print(\"‚û°Ô∏è  Routing to author identification\")\n",
    "        return \"author\"\n",
    "    elif decision.suggestion:\n",
    "        print(\"‚û°Ô∏è  Routing to book suggestions\")\n",
    "        return \"suggestion\"\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Unclear intent, defaulting to suggestions\")\n",
    "        return \"suggestion\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-construction",
   "metadata": {},
   "source": [
    "## 10. Building the Complete Graph\n",
    "\n",
    "Now we'll assemble all components into a working graph.\n",
    "\n",
    "### Graph Architecture:\n",
    "```\n",
    "START ‚Üí [condition] ‚Üí author_node ‚Üí END\n",
    "                   ‚Üí suggestion_node ‚Üí END\n",
    "```\n",
    "\n",
    "### Key Design Decisions:\n",
    "- **Single Entry Point**: All requests start at the same place\n",
    "- **Dynamic Routing**: LLM determines the appropriate path\n",
    "- **Parallel Paths**: Different nodes for different capabilities\n",
    "- **Clean Termination**: Both paths lead to END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "graph = StateGraph(BookState)\n",
    "\n",
    "# Add our processing nodes\n",
    "graph.add_node(\"author_node\", author_node)\n",
    "graph.add_node(\"suggestion_node\", suggestion_node)\n",
    "\n",
    "# Add conditional routing from START\n",
    "graph.add_conditional_edges(\n",
    "    START,           # From the start\n",
    "    condition,       # Use this function to decide\n",
    "    {\n",
    "        \"author\": \"author_node\",           # Route to author identification\n",
    "        \"suggestion\": \"suggestion_node\"   # Route to book suggestions\n",
    "    }\n",
    ")\n",
    "\n",
    "# Both paths end the graph\n",
    "graph.add_edge(\"author_node\", END)\n",
    "graph.add_edge(\"suggestion_node\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"‚úÖ Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## 11. Graph Visualization\n",
    "\n",
    "Let's visualize our graph structure to understand the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "    print(\"\\nGraph Structure:\")\n",
    "    print(\"START ‚Üí [condition function]\")\n",
    "    print(\"  ‚îú‚îÄ 'author' ‚Üí author_node ‚Üí END\")\n",
    "    print(\"  ‚îî‚îÄ 'suggestion' ‚Üí suggestion_node ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing-section",
   "metadata": {},
   "source": [
    "## 12. Testing the Complete System\n",
    "\n",
    "Let's test both paths of our graph with different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-author-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Author identification\n",
    "print(\"üß™ Test 1: Author Identification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "author_query = {\n",
    "    \"messages\": [HumanMessage(content=\"Who is the author of Khushwantnama?\")]\n",
    "}\n",
    "\n",
    "result1 = app.invoke(author_query)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Author: {result1.get('author_name', 'Not found')}\")\n",
    "print(f\"Persona: {result1.get('author_persona', 'Not available')}\")\n",
    "print(f\"Suggestions: {result1.get('suggestions', 'None')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-suggestion-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Book suggestions\n",
    "print(\"\\nüß™ Test 2: Book Suggestions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "suggestion_query = {\n",
    "    \"messages\": [HumanMessage(content=\"Suggest some books like Macbeth\")]\n",
    "}\n",
    "\n",
    "result2 = app.invoke(suggestion_query)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Author: {result2.get('author_name', 'Not found')}\")\n",
    "print(f\"Persona: {result2.get('author_persona', 'Not available')}\")\n",
    "print(f\"Suggestions: {result2.get('suggestions', 'None')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-edge-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Edge case - ambiguous query\n",
    "print(\"\\nüß™ Test 3: Ambiguous Query\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ambiguous_query = {\n",
    "    \"messages\": [HumanMessage(content=\"Tell me about books\")]\n",
    "}\n",
    "\n",
    "result3 = app.invoke(ambiguous_query)\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Author: {result3.get('author_name', 'Not found')}\")\n",
    "print(f\"Persona: {result3.get('author_persona', 'Not available')}\")\n",
    "print(f\"Suggestions: {result3.get('suggestions', 'None')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Structured Output**: Using Pydantic models for reliable LLM responses\n",
    "2. **Conditional Routing**: Dynamic graph paths based on user intent\n",
    "3. **Multi-Node Coordination**: Building complex workflows with multiple capabilities\n",
    "4. **LLM Integration**: Working with AWS Bedrock and Claude effectively\n",
    "5. **Intent Classification**: Using LLMs to route between different services\n",
    "6. **State Management**: Handling complex state with multiple data types\n",
    "\n",
    "### When to Use This Pattern:\n",
    "- **Multi-Service Applications**: When you have different capabilities to offer\n",
    "- **Intent-Based Routing**: When user requests need different processing\n",
    "- **Structured Data Extraction**: When you need consistent, typed responses\n",
    "- **Conversational AI**: When building chatbots with multiple skills\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "- **Modular Design**: Separate chains for different capabilities\n",
    "- **Type Safety**: Using Pydantic for structured responses\n",
    "- **Clear Separation**: Distinct nodes for different processing types\n",
    "- **Robust Routing**: LLM-based intent classification\n",
    "- **Error Handling**: Graceful fallbacks for unclear intents\n",
    "\n",
    "### Common Pitfalls to Avoid:\n",
    "- **Wrong Return Format**: Nodes must return state update dictionaries\n",
    "- **Missing State Fields**: Ensure state includes all needed fields\n",
    "- **Poor Prompts**: Unclear prompts lead to inconsistent routing\n",
    "- **No Fallbacks**: Always handle edge cases in routing logic\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 3**: Build simple chatbots with LangGraph\n",
    "- **Notebook 4**: Add memory and persistence to conversations\n",
    "- **Notebook 5**: Implement SQLite-based persistent memory\n",
    "\n",
    "### Extensions You Could Add:\n",
    "- **More Services**: Add genre classification, book reviews, etc.\n",
    "- **Better Routing**: Multi-level intent classification\n",
    "- **Error Recovery**: Retry logic for failed LLM calls\n",
    "- **Caching**: Store frequent queries for faster responses\n",
    "- **User Profiles**: Personalized recommendations based on history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
