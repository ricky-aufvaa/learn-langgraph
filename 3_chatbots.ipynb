{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Basic Chatbots with LangGraph\n",
    "\n",
    "In this notebook, I'll walk through building simple conversational chatbots using LangGraph. This is where I started getting comfortable with the core concepts:\n",
    "\n",
    "- **MessagesState**: Built-in state for conversation management\n",
    "- **Simple Architecture**: Minimal setup for chatbot functionality\n",
    "- **LLM Integration**: Direct integration with language models\n",
    "- **Conversation Flow**: Basic request-response patterns\n",
    "- **Stateless Design**: Each interaction is independent\n",
    "\n",
    "## What I'm Building\n",
    "\n",
    "I'll create a basic chatbot that:\n",
    "1. **Processes User Messages**: Handles text input from users\n",
    "2. **Generates Responses**: Uses LLM to create appropriate replies\n",
    "3. **Maintains Context**: Keeps track of the current conversation\n",
    "4. **Simple Flow**: Straightforward request-response pattern\n",
    "\n",
    "This approach works well for:\n",
    "- **Learning LangGraph**: Understanding core concepts\n",
    "- **Prototyping**: Quick chatbot development\n",
    "- **Simple Use Cases**: FAQ bots, basic assistants\n",
    "- **Foundation Building**: Base for more complex applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Essential Imports\n",
    "\n",
    "Starting with the core components I need for a basic chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import display, Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Definition for Conversations\n",
    "\n",
    "MessagesState is what I use as the foundation for conversational applications in LangGraph.\n",
    "\n",
    "### Why I chose MessagesState:\n",
    "- **Built-in Support**: Designed specifically for conversations\n",
    "- **Message Handling**: Automatic management of conversation flow\n",
    "- **Type Safety**: Ensures proper message structure\n",
    "- **LangChain Integration**: Works seamlessly with LangChain components\n",
    "\n",
    "### Key Components:\n",
    "- **messages**: List of conversation messages\n",
    "- **add_messages**: Function to append new messages\n",
    "- **Annotated**: Type hint for proper message accumulation\n",
    "\n",
    "### Other approaches I considered:\n",
    "- **Custom State**: More control but requires manual message handling\n",
    "- **Simple Dictionary**: Less structure but more flexibility\n",
    "- **Extended MessagesState**: Add custom fields to the base state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatState(TypedDict):\n",
    "    \"\"\"State definition for my basic chatbot.\n",
    "    \n",
    "    This state manages the conversation messages using LangGraph's\n",
    "    built-in message handling capabilities.\n",
    "    \n",
    "    The Annotated type with add_messages ensures that new messages\n",
    "    are properly appended to the conversation history.\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Configuration\n",
    "\n",
    "I'm using AWS Bedrock with Claude for my language model.\n",
    "\n",
    "### Model Selection - what I learned:\n",
    "- **Claude Sonnet**: Balanced performance and cost\n",
    "- **Claude Haiku**: Faster responses, lower cost\n",
    "- **Claude Opus**: Highest quality for complex tasks\n",
    "\n",
    "### Environment Setup:\n",
    "Make sure your `.env` file contains:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=your_access_key\n",
    "AWS_SECRET_ACCESS_KEY=your_secret_key\n",
    "```\n",
    "\n",
    "### Other LLM options I could use:\n",
    "- **OpenAI**: `ChatOpenAI` for GPT models\n",
    "- **Anthropic**: Direct `ChatAnthropic` integration\n",
    "- **Local Models**: `Ollama` for on-premise deployment\n",
    "- **Azure**: `AzureChatOpenAI` for Microsoft cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"Initialize and return a ChatBedrock model instance.\n",
    "    \n",
    "    This function creates a connection to AWS Bedrock with Claude Sonnet,\n",
    "    which provides a good balance of capability and performance for\n",
    "    conversational applications.\n",
    "    \n",
    "    Returns:\n",
    "        ChatBedrock: Configured language model for conversations\n",
    "    \"\"\"\n",
    "    llm = ChatBedrock(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "print(\"LLM configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering for Chatbots\n",
    "\n",
    "Prompt design is something I had to get right for chatbot behavior and personality.\n",
    "\n",
    "### What I learned about prompt design:\n",
    "- **Clear Role Definition**: Establish what the chatbot is\n",
    "- **Behavior Guidelines**: How it should respond\n",
    "- **Tone and Style**: Personality characteristics\n",
    "- **Conversation Context**: How to handle message history\n",
    "\n",
    "### System Prompt Components:\n",
    "1. **Identity**: Who/what the chatbot is\n",
    "2. **Capabilities**: What it can and cannot do\n",
    "3. **Behavior**: How it should interact\n",
    "4. **Constraints**: Important limitations or guidelines\n",
    "\n",
    "### What works in practice:\n",
    "- **Be Specific**: Clear instructions prevent confusion\n",
    "- **Set Expectations**: Define the chatbot's role clearly\n",
    "- **Include Examples**: Show desired behavior when needed\n",
    "- **Handle Edge Cases**: Address common problematic scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def create_chatbot_chain():\n",
    "    \"\"\"Create a conversation chain for my basic chatbot.\n",
    "    \n",
    "    This chain combines a system prompt that defines the chatbot's\n",
    "    personality and behavior with a placeholder for conversation messages.\n",
    "    \n",
    "    Returns:\n",
    "        Chain: LangChain chain for processing conversations\n",
    "    \"\"\"\n",
    "    llm = get_chat_model()\n",
    "    \n",
    "    # Define the chatbot's personality and behavior\n",
    "    system_prompt = (\n",
    "        \"You are a helpful and friendly chatbot assistant. \"\n",
    "        \"Your goal is to be helpful, informative, and engaging in conversations. \"\n",
    "        \"You should:\"\n",
    "        \"\\n- Provide clear and accurate information when possible\"\n",
    "        \"\\n- Ask clarifying questions when needed\"\n",
    "        \"\\n- Be conversational and maintain a friendly tone\"\n",
    "        \"\\n- Acknowledge when you don't know something\"\n",
    "        \"\\n- Keep responses concise but thorough\"\n",
    "        \"\\n- Remember the context of the conversation\"\n",
    "    )\n",
    "    \n",
    "    # Create the prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    # Combine prompt and LLM into a chain\n",
    "    chain = prompt | llm\n",
    "    \n",
    "    return chain\n",
    "\n",
    "print(\"Chatbot chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot Node Implementation\n",
    "\n",
    "The chatbot node is the core processing unit that handles user messages.\n",
    "\n",
    "### What this node does:\n",
    "1. **Receive State**: Get current conversation state\n",
    "2. **Process Messages**: Use the LLM chain to generate responses\n",
    "3. **Return Updates**: Provide new messages to update the state\n",
    "\n",
    "### Critical things I learned:\n",
    "- **Input**: Must accept the full state as parameter\n",
    "- **Processing**: Use the conversation chain with message history\n",
    "- **Output**: Return dictionary with state updates\n",
    "- **Message Handling**: New messages are automatically appended\n",
    "\n",
    "### Error handling I had to add:\n",
    "- **LLM Failures**: Handle API errors gracefully\n",
    "- **Invalid Input**: Manage malformed messages\n",
    "- **Rate Limits**: Handle API rate limiting\n",
    "- **Timeouts**: Manage slow responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"Process user messages and generate chatbot responses.\n",
    "    \n",
    "    This node takes the current conversation state, processes it through\n",
    "    our chatbot chain, and returns the LLM's response as a new message.\n",
    "    \n",
    "    The add_messages annotation in our state definition ensures that\n",
    "    the response is appended to the conversation history rather than\n",
    "    replacing the existing messages.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state with message history\n",
    "        \n",
    "    Returns:\n",
    "        State update containing the chatbot's response message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the conversation chain\n",
    "        chain = create_chatbot_chain()\n",
    "        \n",
    "        # Process the conversation through the chain\n",
    "        response = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "        \n",
    "        # Return the response as a state update\n",
    "        return {\"messages\": response}\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        print(f\"Error in chatbot_node: {e}\")\n",
    "        \n",
    "        # Return an error message\n",
    "        from langchain_core.messages import AIMessage\n",
    "        error_response = AIMessage(\n",
    "            content=\"I apologize, but I encountered an error processing your message. Please try again.\"\n",
    "        )\n",
    "        return {\"messages\": error_response}\n",
    "\n",
    "print(\"Chatbot node implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building the Chatbot Graph\n",
    "\n",
    "My chatbot uses a simple linear graph structure:\n",
    "\n",
    "```\n",
    "START → chatbot_node → END\n",
    "```\n",
    "\n",
    "### Why I chose this design:\n",
    "- **Simplicity**: Single node handles all conversation logic\n",
    "- **Clarity**: Easy to understand and debug\n",
    "- **Extensibility**: Simple to add more nodes later\n",
    "- **Performance**: Minimal overhead for basic conversations\n",
    "\n",
    "### Why this architecture works:\n",
    "- **Single Responsibility**: One node, one purpose\n",
    "- **State Management**: LangGraph handles message accumulation\n",
    "- **Flexibility**: Easy to modify behavior by changing the node\n",
    "- **Scalability**: Can handle multiple concurrent conversations\n",
    "\n",
    "### Extensions I could add later:\n",
    "- **Intent Classification**: Add routing based on user intent\n",
    "- **Tool Integration**: Add function calling capabilities\n",
    "- **Content Filtering**: Add moderation and safety checks\n",
    "- **Analytics**: Add logging and conversation tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatbot_graph():\n",
    "    \"\"\"Build and compile the chatbot graph.\n",
    "    \n",
    "    This function creates a simple linear graph with a single chatbot node\n",
    "    that processes all user messages and generates appropriate responses.\n",
    "    \n",
    "    Returns:\n",
    "        Compiled graph ready for conversation processing\n",
    "    \"\"\"\n",
    "    # Initialize the graph with our state type\n",
    "    graph = StateGraph(ChatState)\n",
    "    \n",
    "    # Add the chatbot processing node\n",
    "    graph.add_node(\"chatbot\", chatbot_node)\n",
    "    \n",
    "    # Set up the flow: START → chatbot → END\n",
    "    graph.add_edge(START, \"chatbot\")\n",
    "    graph.add_edge(\"chatbot\", END)\n",
    "    \n",
    "    # Compile the graph into an executable application\n",
    "    app = graph.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create our chatbot application\n",
    "chatbot_app = create_chatbot_graph()\n",
    "\n",
    "print(\"Chatbot graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph Visualization\n",
    "\n",
    "Let me visualize this simple but effective chatbot architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(chatbot_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "    print(\"\\nGraph Structure:\")\n",
    "    print(\"START → chatbot_node → END\")\n",
    "    print(\"\\nThis simple linear flow processes user messages and generates responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing the Basic Chatbot\n",
    "\n",
    "Let me test this chatbot with various types of messages to see how it performs.\n",
    "\n",
    "### My test strategy:\n",
    "1. **Simple Greeting**: Basic conversation starter\n",
    "2. **Information Request**: Ask for specific information\n",
    "3. **Follow-up Question**: Test conversation context\n",
    "4. **Complex Query**: More challenging request\n",
    "\n",
    "### What I'm looking for:\n",
    "- **Response Quality**: Appropriate and helpful responses\n",
    "- **Context Awareness**: References to previous messages\n",
    "- **Personality**: Consistent friendly and helpful tone\n",
    "- **Error Handling**: Graceful handling of issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test 1: Simple greeting\n",
    "print(\"Test 1: Simple Greeting\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "greeting_message = \"Hello! How are you today?\"\n",
    "print(f\"User: {greeting_message}\")\n",
    "\n",
    "result = chatbot_app.invoke({\n",
    "    \"messages\": [HumanMessage(content=greeting_message)]\n",
    "})\n",
    "\n",
    "print(f\"Chatbot: {result['messages'][-1].content}\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Information request\n",
    "print(\"Test 2: Information Request\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "info_message = \"Can you explain what LangGraph is and how it works?\"\n",
    "print(f\"User: {info_message}\")\n",
    "\n",
    "result = chatbot_app.invoke({\n",
    "    \"messages\": [HumanMessage(content=info_message)]\n",
    "})\n",
    "\n",
    "print(f\"Chatbot: {result['messages'][-1].content}\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Multi-turn conversation\n",
    "print(\"Test 3: Multi-turn Conversation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start a conversation\n",
    "conversation_messages = [\n",
    "    HumanMessage(content=\"I'm learning about AI and chatbots. Can you help me?\")\n",
    "]\n",
    "\n",
    "print(f\"User: {conversation_messages[0].content}\")\n",
    "\n",
    "# Get first response\n",
    "result = chatbot_app.invoke({\"messages\": conversation_messages})\n",
    "conversation_messages = result[\"messages\"]\n",
    "\n",
    "print(f\"Chatbot: {conversation_messages[-1].content}\")\n",
    "print()\n",
    "\n",
    "# Continue the conversation\n",
    "follow_up = \"What are the main differences between rule-based and AI-powered chatbots?\"\n",
    "conversation_messages.append(HumanMessage(content=follow_up))\n",
    "\n",
    "print(f\"User: {follow_up}\")\n",
    "\n",
    "# Get second response\n",
    "result = chatbot_app.invoke({\"messages\": conversation_messages})\n",
    "conversation_messages = result[\"messages\"]\n",
    "\n",
    "print(f\"Chatbot: {conversation_messages[-1].content}\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Streaming Responses\n",
    "\n",
    "For a more interactive experience, let me implement streaming responses.\n",
    "\n",
    "### Why streaming is useful:\n",
    "- **Real-time Feedback**: Users see responses as they're generated\n",
    "- **Better UX**: Reduces perceived latency\n",
    "- **Interruptible**: Can stop generation if needed\n",
    "- **Progressive Display**: Show partial responses\n",
    "\n",
    "### When I use streaming:\n",
    "- **Long Responses**: When answers might be lengthy\n",
    "- **Interactive Applications**: Real-time chat interfaces\n",
    "- **User Experience**: When responsiveness is important\n",
    "- **Live Demos**: Showing the AI \"thinking\" process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Streaming response\n",
    "print(\"Test 4: Streaming Response\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "streaming_message = \"Tell me a story about a robot learning to be creative.\"\n",
    "print(f\"User: {streaming_message}\")\n",
    "print(\"\\nChatbot (streaming): \", end=\"\")\n",
    "\n",
    "# Use streaming to show response as it's generated\n",
    "for chunk in chatbot_app.stream({\n",
    "    \"messages\": [HumanMessage(content=streaming_message)]\n",
    "}):\n",
    "    # Extract the message content from the chunk\n",
    "    if \"chatbot\" in chunk:\n",
    "        message = chunk[\"chatbot\"][\"messages\"]\n",
    "        if hasattr(message, 'content'):\n",
    "            print(message.content)\n",
    "        else:\n",
    "            print(message)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Chat Session\n",
    "\n",
    "Let me create a simple interactive chat interface to demonstrate the full chatbot experience.\n",
    "\n",
    "### What this interface provides:\n",
    "- **Continuous Conversation**: Multiple exchanges in one session\n",
    "- **Context Preservation**: Maintains conversation history\n",
    "- **User Control**: Easy exit mechanism\n",
    "- **Error Handling**: Graceful error recovery\n",
    "\n",
    "### How to use it:\n",
    "1. Type your message and press Enter\n",
    "2. The chatbot will respond based on the conversation history\n",
    "3. Type 'quit', 'exit', or 'bye' to end the conversation\n",
    "4. The conversation context is maintained throughout the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Run an interactive chat session with the chatbot.\n",
    "    \n",
    "    This function provides a simple command-line interface for\n",
    "    chatting with our LangGraph-based chatbot.\n",
    "    \"\"\"\n",
    "    print(\"Interactive Chatbot Session\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Type your messages and press Enter to chat.\")\n",
    "    print(\"Type 'quit', 'exit', or 'bye' to end the conversation.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_messages = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            # Check for exit conditions\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:\n",
    "                print(\"\\nGoodbye! Thanks for chatting!\")\n",
    "                break\n",
    "            \n",
    "            # Skip empty inputs\n",
    "            if not user_input:\n",
    "                print(\"Please enter a message or type 'quit' to exit.\")\n",
    "                continue\n",
    "            \n",
    "            # Add user message to conversation\n",
    "            conversation_messages.append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # Get chatbot response\n",
    "            print(\"\\nChatbot: \", end=\"\")\n",
    "            \n",
    "            result = chatbot_app.invoke({\"messages\": conversation_messages})\n",
    "            \n",
    "            # Update conversation with the response\n",
    "            conversation_messages = result[\"messages\"]\n",
    "            \n",
    "            # Display the response\n",
    "            print(conversation_messages[-1].content)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nChat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please try again or type 'quit' to exit.\")\n",
    "\n",
    "# Run the interactive chat\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Key Takeaways\n",
    "\n",
    "### What I learned building this:\n",
    "\n",
    "1. **Basic Chatbot Architecture**: Simple linear graph for conversation processing\n",
    "2. **MessagesState**: Built-in state management for conversations\n",
    "3. **LLM Integration**: Direct integration with language models\n",
    "4. **Prompt Engineering**: Designing effective system prompts for chatbots\n",
    "5. **Error Handling**: Graceful handling of failures and edge cases\n",
    "6. **Interactive Interfaces**: Building user-friendly chat experiences\n",
    "\n",
    "### When this pattern works well:\n",
    "\n",
    "- **Simple Chatbots**: Basic question-answering applications\n",
    "- **Prototyping**: Quick development and testing of conversational AI\n",
    "- **Learning**: Understanding LangGraph fundamentals\n",
    "- **Foundation**: Base for more complex conversational applications\n",
    "\n",
    "### Good practices I discovered:\n",
    "\n",
    "- **State Management**: Using TypedDict and add_messages for proper conversation flow\n",
    "- **Error Handling**: Graceful degradation when things go wrong\n",
    "- **Modular Design**: Separate functions for different responsibilities\n",
    "- **User Experience**: Clear interfaces and helpful feedback\n",
    "- **Testing Strategy**: Comprehensive testing of different scenarios\n",
    "\n",
    "### Where this approach falls short:\n",
    "\n",
    "- **No Memory**: Conversations don't persist between sessions\n",
    "- **No Context**: Each invocation is independent\n",
    "- **Limited Capabilities**: No tool use or external integrations\n",
    "- **Simple Routing**: No intent classification or conditional logic\n",
    "\n",
    "### What I'm building next:\n",
    "\n",
    "- **Notebook 4**: Add memory capabilities for persistent conversations\n",
    "- **Notebook 5**: Implement SQLite-based persistent storage\n",
    "- **Advanced Features**: Tool integration, function calling, and external APIs\n",
    "- **Production Deployment**: Scaling and monitoring considerations\n",
    "\n",
    "### Extensions I could add:\n",
    "\n",
    "- **Intent Classification**: Route different types of queries to specialized handlers\n",
    "- **Tool Integration**: Add function calling for external data access\n",
    "- **Content Moderation**: Filter inappropriate content and responses\n",
    "- **Analytics**: Track conversation metrics and user satisfaction\n",
    "- **Personalization**: Adapt responses based on user preferences\n",
    "\n",
    "### Architecture patterns I've explored:\n",
    "\n",
    "- **Linear Flow**: Simple request-response (this notebook)\n",
    "- **Conditional Routing**: Branch based on user intent (Notebook 2)\n",
    "- **Memory-Enhanced**: Persistent conversation history (Notebook 4)\n",
    "- **Tool-Augmented**: External function calling capabilities\n",
    "- **Multi-Agent**: Coordination between multiple AI agents\n",
    "\n",
    "### Production considerations I need to think about:\n",
    "\n",
    "- **Scalability**: Consider load balancing for multiple users\n",
    "- **Security**: Implement authentication and input validation\n",
    "- **Monitoring**: Add logging and performance metrics\n",
    "- **Rate Limiting**: Prevent abuse and manage costs\n",
    "- **Error Recovery**: Implement retry logic and fallback responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
