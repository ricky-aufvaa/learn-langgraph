{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Chatbots with Memory\n",
    "\n",
    "This notebook demonstrates how to build conversational chatbots using LangGraph with memory capabilities:\n",
    "\n",
    "- **MessagesState**: Built-in state management for conversations\n",
    "- **MemorySaver**: In-memory conversation persistence\n",
    "- **Thread Management**: Separate conversation contexts\n",
    "- **Streaming Responses**: Real-time conversation flow\n",
    "- **Simple Architecture**: Minimal setup for maximum functionality\n",
    "\n",
    "## Use Case: Basic Conversational AI\n",
    "\n",
    "We'll build a chatbot that:\n",
    "1. **Remembers Context**: Maintains conversation history\n",
    "2. **Handles Multiple Users**: Uses thread IDs for separation\n",
    "3. **Streams Responses**: Provides real-time interaction\n",
    "4. **Persists Memory**: Keeps conversations across sessions\n",
    "\n",
    "This pattern is essential for:\n",
    "- **Customer Support Bots**: Need conversation context\n",
    "- **Personal Assistants**: Remember user preferences and history\n",
    "- **Educational Chatbots**: Track learning progress\n",
    "- **Multi-user Applications**: Separate conversation contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Essential Imports\n",
    "\n",
    "Let's import the core components needed for our memory-enabled chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import display, Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Definition with MessagesState\n",
    "\n",
    "**MessagesState** is LangGraph's built-in state type for conversational applications.\n",
    "\n",
    "### Key Features:\n",
    "- **Automatic Message Handling**: Built-in support for conversation flow\n",
    "- **Message Accumulation**: Uses `add_messages` to append new messages\n",
    "- **Type Safety**: Ensures proper message structure\n",
    "- **Memory Integration**: Works seamlessly with checkpointers\n",
    "\n",
    "### Why Annotated with add_messages?\n",
    "- **Accumulation Logic**: New messages are added to existing ones, not replaced\n",
    "- **Conversation Flow**: Maintains the full conversation history\n",
    "- **Memory Efficiency**: Handles message deduplication automatically\n",
    "\n",
    "### Alternative Approaches:\n",
    "- **Custom State**: More control but requires manual message handling\n",
    "- **Simple List**: No automatic accumulation logic\n",
    "- **MessagesState Inheritance**: Extend with additional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyState(TypedDict):\n",
    "    \"\"\"State definition for our conversational chatbot.\n",
    "    \n",
    "    Uses Annotated with add_messages to ensure new messages\n",
    "    are appended to the conversation history rather than\n",
    "    replacing the entire message list.\n",
    "    \"\"\"\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Setup\n",
    "\n",
    "We'll configure our language model using AWS Bedrock with Claude.\n",
    "\n",
    "### Environment Setup:\n",
    "Make sure your `.env` file contains:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=your_access_key\n",
    "AWS_SECRET_ACCESS_KEY=your_secret_key\n",
    "```\n",
    "\n",
    "### Model Selection:\n",
    "- **Claude Sonnet**: Good balance of capability and speed\n",
    "- **Claude Haiku**: Faster responses, lower cost\n",
    "- **Claude Opus**: Highest capability for complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_chat_model():\n",
    "    \"\"\"Initialize AWS Bedrock ChatBedrock model.\n",
    "    \n",
    "    Returns:\n",
    "        ChatBedrock: Configured LLM instance for conversations\n",
    "    \"\"\"\n",
    "    llm = ChatBedrock(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversation Chain\n",
    "\n",
    "The **conversation chain** combines our prompt template with the LLM to create a reusable conversation component.\n",
    "\n",
    "### Chain Components:\n",
    "1. **System Prompt**: Defines the chatbot's personality and behavior\n",
    "2. **MessagesPlaceholder**: Inserts the conversation history\n",
    "3. **LLM**: Processes the prompt and generates responses\n",
    "\n",
    "### Design Considerations:\n",
    "- **Simple System Prompt**: Clear, concise instructions\n",
    "- **Flexible Placeholder**: Handles variable-length conversations\n",
    "- **Stateless Chain**: No internal state, relies on input messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def conversation_chain():\n",
    "    \"\"\"Create a conversation chain for chatbot responses.\n",
    "    \n",
    "    This chain:\n",
    "    1. Takes the full conversation history\n",
    "    2. Applies a system prompt for consistent behavior\n",
    "    3. Generates contextually appropriate responses\n",
    "    \n",
    "    Returns:\n",
    "        Chain that processes conversation messages\n",
    "    \"\"\"\n",
    "    llm = get_chat_model()\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful chatbot that answers user queries. \"\n",
    "                  \"Be conversational, friendly, and remember the context of our conversation. \"\n",
    "                  \"If users share personal information, acknowledge and remember it.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    return prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation Node\n",
    "\n",
    "The **conversation node** is our graph's processing unit that handles user messages.\n",
    "\n",
    "### Node Responsibilities:\n",
    "1. **Receive State**: Gets current conversation state\n",
    "2. **Process Messages**: Uses the conversation chain\n",
    "3. **Return Updates**: Provides new message to add to history\n",
    "\n",
    "### Critical Implementation Details:\n",
    "- **State Parameter**: Must accept the full state\n",
    "- **Return Format**: Must return a dictionary with state updates\n",
    "- **Message Handling**: Returns the LLM response as a new message\n",
    "\n",
    "### Why This Works:\n",
    "- **Automatic Accumulation**: `add_messages` appends the response\n",
    "- **Context Preservation**: Full conversation history is maintained\n",
    "- **Stateless Processing**: Node doesn't need to manage history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_node(state: MyState) -> MyState:\n",
    "    \"\"\"Process conversation messages and generate responses.\n",
    "    \n",
    "    This node:\n",
    "    1. Takes the current conversation state\n",
    "    2. Passes all messages to the conversation chain\n",
    "    3. Returns the LLM's response as a new message\n",
    "    \n",
    "    The add_messages annotation ensures the response is appended\n",
    "    to the conversation history, not replacing it.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state with conversation messages\n",
    "        \n",
    "    Returns:\n",
    "        State update with the chatbot's response message\n",
    "    \"\"\"\n",
    "    response = conversation_chain().invoke({\"messages\": state[\"messages\"]})\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Configuration\n",
    "\n",
    "**MemorySaver** provides in-memory persistence for conversations.\n",
    "\n",
    "### Memory Types in LangGraph:\n",
    "1. **MemorySaver**: In-memory storage (this notebook)\n",
    "2. **SqliteSaver**: Persistent SQLite storage (next notebook)\n",
    "3. **RedisSaver**: Distributed Redis storage\n",
    "4. **Custom Savers**: Implement your own storage backend\n",
    "\n",
    "### Thread Configuration:\n",
    "- **thread_id**: Unique identifier for each conversation\n",
    "- **Isolation**: Different threads maintain separate conversations\n",
    "- **Scalability**: Support multiple concurrent users\n",
    "\n",
    "### When to Use MemorySaver:\n",
    "- **Development**: Quick testing and prototyping\n",
    "- **Single Session**: Conversations don't need to persist\n",
    "- **Simple Applications**: Minimal setup requirements\n",
    "\n",
    "### Limitations:\n",
    "- **No Persistence**: Memory is lost when application restarts\n",
    "- **Single Process**: Can't share across multiple instances\n",
    "- **Memory Usage**: All conversations stored in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize memory saver for conversation persistence\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Configuration for thread management\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"user_1\"  # Unique identifier for this conversation\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\" Memory configuration ready\")\n",
    "print(f\"Thread ID: {config['configurable']['thread_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building the Graph\n",
    "\n",
    "Our chatbot graph is intentionally simple:\n",
    "\n",
    "```\n",
    "START â†’ conversation_node â†’ END\n",
    "```\n",
    "\n",
    "### Graph Architecture:\n",
    "- **Single Node**: All conversation logic in one place\n",
    "- **Linear Flow**: No branching or conditional logic needed\n",
    "- **Memory Integration**: Checkpointer handles state persistence\n",
    "\n",
    "### Why This Simple Design Works:\n",
    "- **Conversation Focus**: Single responsibility (chat responses)\n",
    "- **Memory Handling**: Checkpointer manages conversation history\n",
    "- **Extensibility**: Easy to add more nodes later\n",
    "\n",
    "### Potential Extensions:\n",
    "- **Intent Classification**: Route to different response types\n",
    "- **Tool Integration**: Add function calling capabilities\n",
    "- **Content Filtering**: Add safety and moderation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the conversation graph\n",
    "graph = StateGraph(MyState)\n",
    "\n",
    "# Add our single conversation node\n",
    "graph.add_node(\"conversation\", conversation_node)\n",
    "\n",
    "# Set up the flow: START â†’ conversation â†’ END\n",
    "graph.add_edge(START, \"conversation\")\n",
    "graph.add_edge(\"conversation\", END)\n",
    "\n",
    "# Compile with memory checkpointer\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\" Graph compiled successfully with memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Graph Visualization\n",
    "\n",
    "Let's visualize our simple but powerful chatbot architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "    print(\"\\nGraph Structure:\")\n",
    "    print(\"START â†’ conversation_node â†’ END\")\n",
    "    print(\"(with MemorySaver checkpointer for conversation persistence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing Memory Functionality\n",
    "\n",
    "Let's test our chatbot's memory capabilities with a sequence of related messages.\n",
    "\n",
    "### Test Scenario:\n",
    "1. **Introduction**: User shares their name\n",
    "2. **Memory Check**: Ask if the bot remembers\n",
    "3. **Context Building**: Add more personal information\n",
    "4. **Recall Test**: Verify the bot remembers multiple details\n",
    "\n",
    "### What to Observe:\n",
    "- **Context Retention**: Bot remembers previous messages\n",
    "- **Natural Flow**: Responses build on conversation history\n",
    "- **Personalization**: Bot uses remembered information appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Test 1: Introduction\n",
    "print(\" Test 1: User Introduction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "question1 = \"Hi! My name is Sarabjot and I'm learning LangGraph.\"\n",
    "print(f\"User: {question1}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=question1)]}, config=config):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Memory Check\n",
    "print(\" Test 2: Memory Recall\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "question2 = \"What is my name and what am I learning?\"\n",
    "print(f\"User: {question2}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=question2)]}, config=config):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Additional Context\n",
    "print(\" Test 3: Building Context\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "question3 = \"I'm particularly interested in building chatbots with memory. Can you help me understand the key concepts?\"\n",
    "print(f\"User: {question3}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=question3)]}, config=config):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Comprehensive Recall\n",
    "print(\"Test 4: Comprehensive Memory Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "question4 = \"Can you summarize what you know about me and our conversation so far?\"\n",
    "print(f\"User: {question4}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=question4)]}, config=config):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-User Testing\n",
    "\n",
    "Let's demonstrate how thread IDs enable separate conversations for different users.\n",
    "\n",
    "### Thread Isolation:\n",
    "- **Separate Contexts**: Each thread_id maintains its own conversation\n",
    "- **No Cross-Talk**: Users can't see each other's conversations\n",
    "- **Concurrent Support**: Multiple users can chat simultaneously\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Multi-tenant SaaS**: Separate customer conversations\n",
    "- **Customer Support**: Agent handles multiple tickets\n",
    "- **Educational Platforms**: Individual student progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second user configuration\n",
    "config_user2 = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"user_2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\" Multi-User Test: User 2 Introduction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "user2_message = \"Hello! I'm Alex and I'm new to AI development.\"\n",
    "print(f\"User 2: {user2_message}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=user2_message)]}, config_user2):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that User 1's context is still separate\n",
    "print(\"ðŸ§ª Context Isolation Test: Back to User 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "isolation_test = \"Do you remember my name?\"\n",
    "print(f\"User 1: {isolation_test}\")\n",
    "\n",
    "for event in app.stream({\"messages\": [HumanMessage(content=isolation_test)]}, config):\n",
    "    for value in event.values():\n",
    "        print(f\"Assistant: {value['messages'].content}\")\n",
    "\n",
    "print(\"\\n User 1's context should be preserved (Sarabjot, LangGraph)\")\n",
    "print(\"ser 2's context should be separate (Alex, AI development)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What I Learned:\n",
    "\n",
    "1. **MessagesState**: Built-in conversation state management\n",
    "2. **add_messages**: Automatic message accumulation for conversation flow\n",
    "3. **MemorySaver**: In-memory conversation persistence\n",
    "4. **Thread Management**: Separate conversation contexts per user\n",
    "5. **Simple Architecture**: Minimal setup for maximum functionality\n",
    "6. **Streaming**: Real-time conversation interaction\n",
    "\n",
    "### When to Use This Pattern:\n",
    "\n",
    "- **Conversational AI**: Any chatbot or assistant application\n",
    "- **Customer Support**: Context-aware support interactions\n",
    "- **Educational Tools**: Tutoring bots that remember student progress\n",
    "- **Personal Assistants**: AI that learns user preferences over time\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "\n",
    "- **Type Safety**: Using TypedDict for state definition\n",
    "- **Memory Integration**: Proper checkpointer configuration\n",
    "- **Thread Isolation**: Separate contexts for different users\n",
    "- **Simple Design**: Focus on core functionality first\n",
    "- **Testing Strategy**: Verify memory and isolation capabilities\n",
    "\n",
    "### Limitations of MemorySaver:\n",
    "\n",
    "- **No Persistence**: Conversations lost on restart\n",
    "- **Memory Usage**: All data stored in RAM\n",
    "- **Single Process**: Can't share across multiple instances\n",
    "- **No Backup**: Risk of data loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
